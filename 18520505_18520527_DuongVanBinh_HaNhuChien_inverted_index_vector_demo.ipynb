{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"18520505_18520527_DuongVanBinh_HaNhuChien_inverted_index_vector_demo.ipynb","provenance":[],"mount_file_id":"1D3BL1A5wLYp8znfYlkqicY0xUpiqj7oa","authorship_tag":"ABX9TyPzGWmdgm7/TknjMbOd2Tr1"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"hK1v8R4p1vDd"},"source":["!pip install streamlit\r\n","!pip install pyngrok==4.1.10\r\n","!pip install unidecode"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LUGLkTaI2H1_","executionInfo":{"status":"ok","timestamp":1611653028597,"user_tz":-420,"elapsed":1207,"user":{"displayName":"Binh Duong Van","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggt0R_vpxdkA9SOrPvnHgwuzwsHmYSxz6AMEDHtsA=s64","userId":"14650316510824230211"}},"outputId":"c3ed9090-8419-4822-8e76-e8879336576d"},"source":["%%writefile app.py\r\n","import streamlit as st\r\n","import string, math\r\n","import nltk\r\n","from nltk.tokenize import word_tokenize\r\n","nltk.download('punkt')\r\n","nltk.download('stopwords')\r\n","from nltk.corpus import stopwords\r\n","stoplist = stopwords.words(\"english\")\r\n","from nltk.stem import PorterStemmer\r\n","ps = PorterStemmer()\r\n","import joblib\r\n","import operator\r\n","\r\n","def make_docsList(df):\r\n","  docs = []\r\n","  for i in range(df.shape[0]):\r\n","    docs.append(df.text[i])\r\n","  return docs\r\n","\r\n","def cleaning(docs_list):\r\n","  clean_docs = []\r\n","  for doc in docs_list:\r\n","    words = []\r\n","    try:\r\n","      for word in doc.split(' '):\r\n","        word = word.lower()\r\n","        for char in word:\r\n","          if char in string.punctuation+'1234567890'+'\\n':\r\n","            word = word.replace(char, ' ')\r\n","        words.append(word.replace(' ',''))\r\n","        \r\n","      clean_docs.append(' '.join(w for w in words if w != ''))\r\n","    except:\r\n","      print(doc)\r\n","  return clean_docs\r\n","\r\n","def tokenize(docs_list):\r\n","  tokens_list = []\r\n","  for doc in docs_list:\r\n","    tokens_list.append(word_tokenize(doc))\r\n","\r\n","  return tokens_list\r\n","\r\n","def remove_stopwords(tokens, stoplist):\r\n","  new_tokens = []\r\n","  for token in tokens:\r\n","    temp = []\r\n","    for word in token:\r\n","      if word not in stoplist:\r\n","        temp.append(word)\r\n","    new_tokens.append(temp)\r\n","\r\n","  return new_tokens\r\n","\r\n","def stemming(tokens):\r\n","  new_tokens = []\r\n","  for token in tokens:\r\n","    temp = []\r\n","    for word in token:\r\n","      temp.append(ps.stem(word))\r\n","    new_tokens.append(temp)\r\n","\r\n","  return new_tokens\r\n","\r\n","def doc_stemming(docs):\r\n","  stemmed_docs = []\r\n","  for doc in docs:\r\n","    stemmed = []\r\n","    for word in word_tokenize(doc):\r\n","      stemmed.append(ps.stem(word))\r\n","    stemmed_docs.append(' '.join(w for w in stemmed))\r\n","  return stemmed_docs\r\n","\r\n","def count_docs_appearance(docs_list, tokens_list):\r\n","  terms = []\r\n","  for token in tokens_list:\r\n","    for word in token:\r\n","      terms.append(word)\r\n","  terms = list(set(terms))\r\n","  terms = sorted(terms)\r\n","\r\n","  counting_dict = {}\r\n","  for term in terms:\r\n","    counting_dict[term] = []\r\n","    for id, doc in enumerate(docs_list):\r\n","      if term in doc.split():\r\n","        counting_dict[term].append(id+1)\r\n","  \r\n","  return terms, counting_dict\r\n","\r\n","def count_freq(terms, docs_list):\r\n","  doc_freq = []\r\n","  for term in terms:\r\n","    freq = []\r\n","    for doc in docs_list:\r\n","      count = 0\r\n","      for word in doc.split():\r\n","        if term==word:\r\n","          count += 1\r\n","      if count>0:\r\n","        freq.append(count)\r\n","    doc_freq.append(freq)\r\n","  return doc_freq\r\n","\r\n","def docRetrieval(query, posting_list):\r\n","  query = cleaning(query)\r\n","  stemmed_query = doc_stemming(query)\r\n","  query_token = stemming(remove_stopwords(tokenize(query), stoplist))\r\n","  query_terms, _ = count_docs_appearance(stemmed_query, query_token)\r\n","  query_tf = count_freq(query_terms, stemmed_query)\r\n","\r\n","  temp = query_tf\r\n","  query_tf = []\r\n","  for i in temp:\r\n","    query_tf += i\r\n","\r\n","  query_idf = []\r\n","  for t in query_terms:\r\n","    try:\r\n","      query_idf.append(float(posting_list[posting_list['terms']==t]['idf']))\r\n","    except:\r\n","      query_idf.append(0)\r\n","\r\n","  query_w = []\r\n","  for q_tf, q_idf in zip(query_tf, query_idf):\r\n","    query_w.append(q_tf*q_idf)\r\n","\r\n","  query_norm = sum([w**2 for w in query_w])\r\n","  query_norm = round(math.sqrt(query_norm), 2)\r\n","\r\n","  query_w = [round(w/query_norm, 2) for w in query_w]\r\n","\r\n","  doc_id, sim = [], []\r\n","  for q_t in query_terms:\r\n","    try:\r\n","      doc_id.append(list(posting_list[posting_list['terms']==q_t]['doc_id'])[0])\r\n","      sim.append(list((posting_list[posting_list['terms']==q_t]['normed_w']))[0])\r\n","    except:\r\n","      pass\r\n","\r\n","  similarity = []\r\n","  for i in range(len(sim)):\r\n","    temp = []\r\n","    for w in sim[i]:\r\n","      temp.append(round(w*query_w[i], 2))\r\n","    similarity.append(temp)\r\n","\r\n","  shuffled_dict = dict(zip(doc_id[0], similarity[0]))\r\n","  for i in range(1, len(doc_id)):\r\n","    temp_dict = dict(zip(doc_id[i], similarity[i]))\r\n","    for k, v in temp_dict.items():\r\n","      if k in shuffled_dict.keys():\r\n","        shuffled_dict[k] = round(shuffled_dict[k] + v, 2)\r\n","      else:\r\n","        shuffled_dict[k] = v\r\n","  \r\n","  result_dict = dict(sorted(shuffled_dict.items(), key=operator.itemgetter(1),reverse=True))\r\n","  return list(result_dict.keys())[:35]\r\n","\r\n","\r\n","\r\n","def main():\r\n","  st.title(\"Vector space model on Cranfield\")\r\n","  st.subheader(\"Query: \")\r\n","\r\n","  user_input = st.text_input(\"\", \"airplane spot for example\")\r\n","\r\n","  # load normed posting list  file\r\n","  pl_path = '/content/drive/MyDrive/Colab Notebooks/Truy xuất thông tin - CS419.L11/Report/norm_posting_list.plk'\r\n","  with open(pl_path, 'rb') as f:\r\n","    posting_list = joblib.load(f)\r\n","\r\n","  query = [user_input]\r\n","  result = docRetrieval(query, posting_list)\r\n","  st.subheader(result)\r\n","###\r\n","if __name__ == '__main__':\r\n","  main()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Overwriting app.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umjAHv8U4iTM","executionInfo":{"status":"ok","timestamp":1611652656283,"user_tz":-420,"elapsed":1616,"user":{"displayName":"Binh Duong Van","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggt0R_vpxdkA9SOrPvnHgwuzwsHmYSxz6AMEDHtsA=s64","userId":"14650316510824230211"}},"outputId":"466cccb9-8294-4978-c819-ceaa71797cf2"},"source":["from pyngrok import ngrok\r\n","!streamlit run app.py &>/dev/null&\r\n","!pgrep streamlit"],"execution_count":3,"outputs":[{"output_type":"stream","text":["190\n","237\n","272\n","431\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"qKnX72wz4scS","executionInfo":{"status":"ok","timestamp":1611652658778,"user_tz":-420,"elapsed":1353,"user":{"displayName":"Binh Duong Van","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggt0R_vpxdkA9SOrPvnHgwuzwsHmYSxz6AMEDHtsA=s64","userId":"14650316510824230211"}},"outputId":"4acbdd73-7461-4dbc-a8d7-9194326a25df"},"source":["public_url = ngrok.connect(port='8501')\r\n","public_url"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'http://3ac4f4002f9a.ngrok.io'"]},"metadata":{"tags":[]},"execution_count":4}]}]}